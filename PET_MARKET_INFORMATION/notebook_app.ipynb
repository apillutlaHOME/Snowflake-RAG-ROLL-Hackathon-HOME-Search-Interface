{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cc4mhv4zj5xqv4qpvr2x",
   "authorId": "6188994524081",
   "authorName": "APILLUTLA",
   "authorEmail": "apoorva@imcominghome.io",
   "sessionId": "4f614793-374d-4922-8a4e-bc468e8761bd",
   "lastEditTime": 1737239441548
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 677,
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\n\n   \n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nimport streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\n\nfrom snowflake.cortex import Complete\nfrom snowflake.core import Root\n\nimport pandas as pd\nimport json\n\npd.set_option(\"max_colwidth\",None)\n\n### Default Values\nNUM_CHUNKS = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\nslide_window = 7 # how many last conversations to remember. This is the slide window.\n\n# service parameters\nCORTEX_SEARCH_DATABASE = \"PET_MARKET_ECONOMICS_DATABASE_SEARCH\"\nCORTEX_SEARCH_SCHEMA = \"PUBLIC\"\nCORTEX_SEARCH_SERVICE = \"PET_MARKET_ECONOMICS_SEARCH\"\n######\n######\n\n# columns to query in the service\nCOLUMNS = [\n    \"chunk\",\n    \"relative_path\",\n    \"category\"\n]\n\nsession = get_active_session()\nroot = Root(session)                         \n\nsvc = root.databases[CORTEX_SEARCH_DATABASE].schemas[CORTEX_SEARCH_SCHEMA].cortex_search_services[CORTEX_SEARCH_SERVICE]\n   \n### Functions\n     \ndef config_options():\n\n    st.sidebar.selectbox('Select your model:',(\n                                    'mixtral-8x7b',\n                                    'snowflake-arctic',\n                                    'mistral-large',\n                                    'llama3-8b',\n                                    'llama3-70b',\n                                    'reka-flash',\n                                     'mistral-7b',\n                                     'llama2-70b-chat',\n                                     'gemma-7b'), key=\"model_name\")\n\n    categories = session.table('docs_chunks_table').select('category').distinct().collect()\n\n    cat_list = ['ALL']\n    for cat in categories:\n        cat_list.append(cat.CATEGORY)\n            \n    st.sidebar.selectbox('Select what products you are looking for', cat_list, key = \"category_value\")\n\n    st.sidebar.checkbox('Do you want that I remember the chat history?', key=\"use_chat_history\", value = True)\n\n    st.sidebar.checkbox('Debug: Click to see summary generated of previous conversation', key=\"debug\", value = True)\n    st.sidebar.button(\"Start Over\", key=\"clear_conversation\", on_click=init_messages)\n    st.sidebar.expander(\"Session State\").write(st.session_state)\n\ndef init_messages():\n\n    # Initialize chat history\n    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\ndef get_similar_chunks_search_service(query):\n\n    if st.session_state.category_value == \"ALL\":\n        response = svc.search(query, COLUMNS, limit=NUM_CHUNKS)\n    else: \n        filter_obj = {\"@eq\": {\"category\": st.session_state.category_value} }\n        response = svc.search(query, COLUMNS, filter=filter_obj, limit=NUM_CHUNKS)\n\n    st.sidebar.json(response.json())\n    \n    return response.json()  \n\ndef get_chat_history():\n#Get the history from the st.session_stage.messages according to the slide window parameter\n    \n    chat_history = []\n    \n    start_index = max(0, len(st.session_state.messages) - slide_window)\n    for i in range (start_index , len(st.session_state.messages) -1):\n         chat_history.append(st.session_state.messages[i])\n\n    return chat_history\n\ndef summarize_question_with_history(chat_history, question):\n# To get the right context, use the LLM to first summarize the previous conversation\n# This will be used to get embeddings and find similar chunks in the docs for context\n\n    prompt = f\"\"\"\n        Based on the chat history below and the question, generate a query that extend the question\n        with the chat history provided. The query should be in natual language. \n        Answer with only the query. Do not add any explanation.\n        \n        <chat_history>\n        {chat_history}\n        </chat_history>\n        <question>\n        {question}\n        </question>\n        \"\"\"\n    \n    sumary = Complete(st.session_state.model_name, prompt)   \n\n    if st.session_state.debug:\n        st.sidebar.text(\"Summary to be used to find similar chunks in the docs:\")\n        st.sidebar.caption(sumary)\n\n    sumary = sumary.replace(\"'\", \"\")\n\n    return sumary\n\ndef create_prompt (myquestion):\n\n    if st.session_state.use_chat_history:\n        chat_history = get_chat_history()\n\n        if chat_history != []: #There is chat_history, so not first question\n            question_summary = summarize_question_with_history(chat_history, myquestion)\n            prompt_context =  get_similar_chunks_search_service(question_summary)\n        else:\n            prompt_context = get_similar_chunks_search_service(myquestion) #First question when using history\n    else:\n        prompt_context = get_similar_chunks_search_service(myquestion)\n        chat_history = \"\"\n  \n    prompt = f\"\"\"\n           You are an expert chat assistance that extracs information from the CONTEXT provided\n           between <context> and </context> tags.\n           You offer a chat experience considering the information included in the CHAT HISTORY\n           provided between <chat_history> and </chat_history> tags..\n           When ansering the question contained between <question> and </question> tags\n           be concise and do not hallucinate. \n           If you donÂ´t have the information just say so.\n           \n           Do not mention the CONTEXT used in your answer.\n           Do not mention the CHAT HISTORY used in your asnwer.\n\n           Only anwer the question if you can extract it from the CONTEXT provideed.\n           \n           <chat_history>\n           {chat_history}\n           </chat_history>\n           <context>          \n           {prompt_context}\n           </context>\n           <question>  \n           {myquestion}\n           </question>\n           Answer: \n           \"\"\"\n    \n    json_data = json.loads(prompt_context)\n\n    relative_paths = set(item['relative_path'] for item in json_data['results'])\n\n    return prompt, relative_paths\n\n\ndef answer_question(myquestion):\n\n    prompt, relative_paths =create_prompt (myquestion)\n\n    response = Complete(st.session_state.model_name, prompt)   \n\n    return response, relative_paths\n\ndef main():\n    \n    st.title(f\":speech_balloon: Chat Document Assistant with Snowflake Cortex\")\n    st.write(\"This is the list of documents you already have and that will be used to answer your questions:\")\n    docs_available = session.sql(\"ls @docs\").collect()\n    list_docs = []\n    for doc in docs_available:\n        list_docs.append(doc[\"name\"])\n    st.dataframe(list_docs)\n\n    config_options()\n    init_messages()\n     \n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n    \n    # Accept user input\n    if question := st.chat_input(\"What do you want to know about your products?\"):\n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.markdown(question)\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n    \n            question = question.replace(\"'\",\"\")\n    \n            with st.spinner(f\"{st.session_state.model_name} thinking...\"):\n                response, relative_paths = answer_question(question)            \n                response = response.replace(\"'\", \"\")\n                message_placeholder.markdown(response)\n\n                if relative_paths != \"None\":\n                    with st.sidebar.expander(\"Related Documents\"):\n                        for path in relative_paths:\n                            cmd2 = f\"select GET_PRESIGNED_URL(@docs, '{path}', 360) as URL_LINK from directory(@docs)\"\n                            df_url_link = session.sql(cmd2).to_pandas()\n                            url_link = df_url_link._get_value(0,'URL_LINK')\n                \n                            display_url = f\"Doc: [{path}]({url_link})\"\n                            st.sidebar.markdown(display_url)\n\n        \n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n\nif __name__ == \"__main__\":\n    main()\n\nsession = get_active_session()\nquery = session.sql(\"SELECT * FROM PET_MARKET_ECONOMICS_DATABASE_SEARCH.PUBLIC.DOCS_CHUNKS_TABLE\")\nresults = query.collect()\ndf = pd.DataFrame(results)\n\n# Show chart type selection\nchart_type = st.selectbox('Choose Chart Type', ['Scatter', 'Bar', 'Line'])\n\nif st.button('Generate Chart'):\n    if chart_type == 'Scatter':\n        fig = px.scatter(df, x='pet industry trends', y='price', title='Service Prices')\n    elif chart_type == 'Bar':\n        fig = px.bar(df, x='market size', y='years', title='Pet Market Size')\n    # Add more conditions for other chart types\n    else:\n        # Handle other chart types or give a default message\n        st.write(\"Please select a chart type.\")\n    \n    st.plotly_chart(fig)\n\n",
   "execution_count": null,
   "outputs": []
  }
 ]
}